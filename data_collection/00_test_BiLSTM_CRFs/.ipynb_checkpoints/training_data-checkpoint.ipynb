{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.6\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4\n",
      "3.7.4 (default, Aug 13 2019, 15:17:50) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "# Check version runing on Jupyter notebook\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "\n",
    "# Check version inside your Python program\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "# Check version in command line or shell\n",
    "# python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.tag import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import glob\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Save / Load File\n",
    "import dill\n",
    "import pickle\n",
    "\n",
    "# Plot Graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn Report\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "\n",
    "# Load Vectors\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Utility\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Model Utility\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Keras Model\n",
    "# from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_contrib.losses import crf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"02+03_dup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#จัดการประโยคซ้ำ\n",
    "data_not=[]\n",
    "def Unique(p):\n",
    "    text=re.sub(\"<[^>]*>\",\"\",p)\n",
    "    text=re.sub(\"\\[(.*?)\\]\",\"\",text)\n",
    "    text=re.sub(\"\\[\\/(.*?)\\]\",\"\",text)\n",
    "    if text not in data_not:\n",
    "        data_not.append(text)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# เตรียมตัวตัด tag ด้วย re\n",
    "pattern = r'\\[(.*?)\\](.*?)\\[\\/(.*?)\\]'\n",
    "tokenizer = RegexpTokenizer(pattern) # ใช้ nltk.tokenize.RegexpTokenizer เพื่อตัด [TIME]8.00[/TIME] ให้เป็น ('TIME','ไง','TIME')\n",
    "# จัดการกับ tag ที่ไม่ได้ tag\n",
    "def toolner_to_tag(text):\n",
    "    text=text.strip()\n",
    "    text=re.sub(\"<[^>]*>\",\"\",text)\n",
    "    text=re.sub(\"(\\[\\/(.*?)\\])\",\"\\\\1***\",text)#.replace('(\\[(.*?)\\])','***\\\\1')#  ตัดการกับพวกไม่มี tag word\n",
    "    text=re.sub(\"(\\[\\w+\\])\",\"***\\\\1\",text)\n",
    "    text2=[]\n",
    "    for i in text.split('***'):\n",
    "        if \"[\" in i:\n",
    "            text2.append(i)\n",
    "        else:\n",
    "            text2.append(\"[word]\"+i+\"[/word]\")\n",
    "            text=\"\".join(text2)#re.sub(\"[word][/word]\",\"\",\"\".join(text2))\n",
    "            return text.replace(\"[word][/word]\",\"\")\n",
    "# แปลง text ให้เป็น conll2002\n",
    "def text2conll2002(text,pos=True):\n",
    "    \"\"\"\n",
    "    ใช้แปลงข้อความให้กลายเป็น conll2002\n",
    "    \"\"\"\n",
    "    text=toolner_to_tag(text) # นำไปใส่ tag [word]\n",
    "    text=text.replace(\"''\",'\"')\n",
    "    text=text.replace(\"’\",'\"').replace(\"‘\",'\"')#.replace('\"',\"\")\n",
    "    tag=tokenizer.tokenize(text) # แยก tag ออกมาจากข้อความ\n",
    "    j=0\n",
    "    conll2002=\"\" # ประกาศตัวแปรเก็บ conll2002\n",
    "    for tagopen,text,tagclose in tag: # ลูปใน tag โดยเป็น (tagopen,text,tagclose)\n",
    "        word_cut=word_tokenize(text,engine=\"newmm\") # ใช้ตัวตัดคำ newmm ของ PyThaiNLP\n",
    "        i=0\n",
    "        txt5=\"\"\n",
    "        while i<len(word_cut): #ลูปตามจำนวน token ที่ตัดในtag\n",
    "            if word_cut[i]==\"''\" or word_cut[i]=='\"':pass\n",
    "            elif i==0 and tagopen!='word': # ไม่เป็น tag [word] และเป็น i หรือตัวเริ่มต้น tag\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'B-'+tagopen\n",
    "            elif tagopen!='word':\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'I-'+tagopen\n",
    "            else: # เป็น [word]\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'O'\n",
    "            txt5+='\\n'\n",
    "            #j+=1\n",
    "            i+=1\n",
    "        conll2002+=txt5\n",
    "    if pos==False:\n",
    "        return conll2002\n",
    "    return postag(conll2002) # เพิ่ม postag ใส่\n",
    "# ใช้สำหรับกำกับ pos tag เพื่อใช้กับ NER\n",
    "# print(text2conll2002(t,pos=False))\n",
    "def postag(text):\n",
    "    listtxt=[i for i in text.split('\\n') if i!='']\n",
    "    list_word=[]\n",
    "    for data in listtxt:\n",
    "        list_word.append(data.split('\\t')[0])\n",
    "    #print(text)\n",
    "    list_word=pos_tag(list_word,engine=\"perceptron\")\n",
    "    text=\"\"\n",
    "    i=0\n",
    "    for data in listtxt:\n",
    "        text+=data.split('\\t')[0]+'\\t'+list_word[i][1]+'\\t'+data.split('\\t')[1]+'\\n'\n",
    "        i+=1\n",
    "    return text\n",
    "# อ่านข้อมูลจากไฟล์\n",
    "def get_data(fileopen):\n",
    "\t\"\"\"\n",
    "    สำหรับใช้อ่านทั้งหมดทั้งในไฟล์ทีละรรทัดออกมาเป็น list\n",
    "    \"\"\"\n",
    "\twith codecs.open(fileopen, 'r',encoding='utf-8-sig') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\treturn [a for a in lines if Unique(a)] # เอาไม่ซ้ำกัน\n",
    "\n",
    "def alldata(lists):\n",
    "    text=\"\"\n",
    "    for data in lists:\n",
    "        text+=text2conll2002(data)\n",
    "        text+='\\n'\n",
    "    return text\n",
    "\n",
    "def alldata_list(lists):\n",
    "    data_all=[]\n",
    "    for data in lists:\n",
    "        data_num=[]\n",
    "        try:\n",
    "            txt=text2conll2002(data,pos=True).split('\\n') # นำไปแปลงเป็น conll2002\n",
    "            for d in txt:\n",
    "                tt=d.split('\\t')\n",
    "                if d!=\"\":\n",
    "                    if len(tt)==3:\n",
    "                        data_num.append((tt[0],tt[1],tt[2]))\n",
    "                    else:\n",
    "                        data_num.append((tt[0],tt[1]))\n",
    "            #print(data_num)\n",
    "            data_all.append(data_num)\n",
    "        except:\n",
    "            print(data)\n",
    "    #print(data_all)\n",
    "    return data_all\n",
    "\n",
    "def alldata_list_str(lists):\n",
    "\tstring=\"\"\n",
    "\tfor data in lists:\n",
    "\t\tstring1=\"\"\n",
    "\t\tfor j in data:\n",
    "\t\t\tstring1+=j[0]+\"\t\"+j[1]+\"\t\"+j[2]+\"\\n\"\n",
    "\t\tstring1+=\"\\n\"\n",
    "\t\tstring+=string1\n",
    "\treturn string\n",
    "\n",
    "def get_data_tag(listd):\n",
    "\tlist_all=[]\n",
    "\tc=[]\n",
    "\tfor i in listd:\n",
    "\t\tif i !='':\n",
    "\t\t\tc.append((i.split(\"\\t\")[0],i.split(\"\\t\")[1],i.split(\"\\t\")[2]))\n",
    "\t\telse:\n",
    "\t\t\tlist_all.append(c)\n",
    "\t\t\tc=[]\n",
    "\treturn list_all\n",
    "def getall(lista):\n",
    "    ll=[]\n",
    "    for i in lista:\n",
    "        o=True\n",
    "        for j in ll:\n",
    "            if re.sub(\"\\[(.*?)\\]\",\"\",i)==re.sub(\"\\[(.*?)\\]\",\"\",j):\n",
    "                o=False\n",
    "                break\n",
    "        if o==True:\n",
    "            ll.append(i)\n",
    "    return ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=getall(get_data(file_name+\".txt\")) # นำคลังเข้าไป แยกออกเป็น list ละบรรทัด\n",
    "datatofile=alldata_list(data1) # นำไปผ่านขั้นตอน 1 2 3 4\n",
    "    \n",
    "tagged_sents = []\n",
    "for i in datatofile:\n",
    "    text_inside = []\n",
    "    for j in i:\n",
    "        text_inside.append((j[0],j[2]))\n",
    "    tagged_sents.append(text_inside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, test_sents= train_test_split(tagged_sents, test_size=0.2, random_state=112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai2fit_model = KeyedVectors.load_word2vec_format('thai2vecNoSym.bin',binary=True)\n",
    "thai2fit_weight = thai2fit_model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=[]\n",
    "ner_list=[]\n",
    "thai2dict = {}\n",
    "\n",
    "for sent in train_sents:\n",
    "    for word in sent:\n",
    "        word_list.append(word[0])\n",
    "        ner_list.append(word[1])\n",
    "        \n",
    "for word in thai2fit_model.index2word:\n",
    "    thai2dict[word] = thai2fit_model[word]\n",
    "\n",
    "word_list.append(\"pad\")\n",
    "word_list.append(\"unknown\") #Special Token for Unknown words (\"UNK\")\n",
    "ner_list.append(\"pad\")\n",
    "\n",
    "all_words = sorted(set(word_list))\n",
    "all_ner = sorted(set(ner_list))\n",
    "all_thai2dict = sorted(set(thai2dict))\n",
    "\n",
    "word_to_ix = dict((c, i) for i, c in enumerate(all_words)) #convert word to index \n",
    "ner_to_ix = dict((c, i) for i, c in enumerate(all_ner)) #convert ner to index\n",
    "thai2dict_to_ix = dict((c, i) for i, c in enumerate(thai2dict)) #convert thai2fit to index \n",
    "\n",
    "ix_to_word = dict((v,k) for k,v in word_to_ix.items()) #convert index to word\n",
    "ix_to_ner = dict((v,k) for k,v in ner_to_ix.items())  #convert index to ner\n",
    "ix_to_thai2dict = dict((v,k) for k,v in thai2dict_to_ix.items())  #convert index to thai2fit\n",
    "\n",
    "n_word = len(word_to_ix)\n",
    "n_tag = len(ner_to_ix)\n",
    "n_thai2dict = len(thai2dict_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set([w_i for w in thai2dict for w_i in w])\n",
    "char2idx = {c: i + 5 for i, c in enumerate(chars)}\n",
    "\n",
    "char2idx[\"pad\"] = 0\n",
    "char2idx[\"unknown\"] = 1\n",
    "char2idx[\" \"] = 2\n",
    "\n",
    "char2idx[\"$\"] = 3\n",
    "char2idx[\"#\"] = 4\n",
    "char2idx[\"!\"] = 5\n",
    "char2idx[\"%\"] = 6\n",
    "char2idx[\"&\"] = 7\n",
    "char2idx[\"*\"] = 8\n",
    "char2idx[\"+\"] = 9\n",
    "char2idx[\",\"] = 10\n",
    "char2idx[\"-\"] = 11\n",
    "char2idx[\".\"] = 12\n",
    "char2idx[\"/\"] = 13\n",
    "char2idx[\":\"] = 14\n",
    "char2idx[\";\"] = 15\n",
    "char2idx[\"?\"] = 16\n",
    "char2idx[\"@\"] = 17\n",
    "char2idx[\"^\"] = 18\n",
    "char2idx[\"_\"] = 19\n",
    "char2idx[\"`\"] = 20\n",
    "char2idx[\"=\"] = 21\n",
    "char2idx[\"|\"] = 22\n",
    "char2idx[\"~\"] = 23\n",
    "char2idx[\"'\"] = 24\n",
    "char2idx['\"'] = 25\n",
    "\n",
    "char2idx[\"(\"] = 26\n",
    "char2idx[\")\"] = 27\n",
    "char2idx[\"{\"] = 28\n",
    "char2idx[\"}\"] = 29\n",
    "char2idx[\"<\"] = 30\n",
    "char2idx[\">\"] = 31\n",
    "char2idx[\"[\"] = 32\n",
    "char2idx[\"]\"] = 33\n",
    "\n",
    "n_chars = len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('chardict.pickle', 'wb') as chardict:\n",
    "#     pickle.dump(char2idx, chardict)\n",
    "# with open('nerdict.pickle', 'wb') as nerdict:\n",
    "#     pickle.dump(ner_to_ix, nerdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 250\n",
    "MAX_LEN = 250\n",
    "max_len_char = 30\n",
    "\n",
    "character_LSTM_unit = 32\n",
    "char_embedding_dim = 32\n",
    "main_lstm_unit = 256 ## Bidirectional 256 + 256 = 512\n",
    "lstm_recurrent_dropout = 0.5\n",
    "\n",
    "train_batch_size = 32\n",
    "train_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_word(input_text):\n",
    "    idxs = list()\n",
    "    for word in input_text:\n",
    "        if word in thai2dict:\n",
    "            idxs.append(thai2dict_to_ix[word])\n",
    "        else:\n",
    "            idxs.append(thai2dict_to_ix[\"unknown\"]) #Use UNK tag for unknown word\n",
    "    return idxs\n",
    "\n",
    "def prepare_sequence_target(input_label):\n",
    "    idxs = [ner_to_ix[w] for w in input_label]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sent =[ [ word[0] for word in sent]for sent in train_sents ] #words only\n",
    "train_targets =[ [ word[1] for word in sent]for sent in train_sents ] #NER only\n",
    "\n",
    "input_test_sent =[ [ word[0] for word in sent]for sent in test_sents ] #words only\n",
    "test_targets =[ [ word[1] for word in sent]for sent in test_sents ] #NER only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word Training\n",
    "X_word_tr = [prepare_sequence_word(s) for s in input_sent]\n",
    "X_word_tr = pad_sequences(maxlen=max_len, sequences=X_word_tr, value=thai2dict_to_ix[\"pad\"], padding='post', truncating='post')\n",
    "\n",
    "## Character Training\n",
    "X_char_tr = []\n",
    "for sentence in train_sents:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                if(sentence[i][0][j] in char2idx):\n",
    "                    word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "                else:\n",
    "                    word_seq.append(char2idx.get(\"unknown\"))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"pad\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    X_char_tr.append(np.array(sent_seq))\n",
    "\n",
    "## Sequence Label Training\n",
    "y_tr = [prepare_sequence_target(s) for s in train_targets]\n",
    "y_tr = pad_sequences(maxlen=max_len, sequences=y_tr, value=ner_to_ix[\"pad\"], padding='post', truncating='post')\n",
    "y_tr = [to_categorical(i, num_classes=n_tag) for i in y_tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word Testing\n",
    "X_word_te = [prepare_sequence_word(s) for s in input_test_sent]\n",
    "X_word_te = pad_sequences(maxlen=max_len, sequences=X_word_te, value=thai2dict_to_ix[\"pad\"], padding='post', truncating='post')\n",
    "\n",
    "## Character Testing\n",
    "X_char_te = []\n",
    "for sentence in test_sents:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                if(sentence[i][0][j] in char2idx):\n",
    "                    word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "                else:\n",
    "                    word_seq.append(char2idx.get(\"unknown\"))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"pad\"))    \n",
    "        sent_seq.append(word_seq)\n",
    "    X_char_te.append(np.array(sent_seq))\n",
    "\n",
    "## Sequence Label Testing\n",
    "y_te = [prepare_sequence_target(s) for s in test_targets]\n",
    "y_te = pad_sequences(maxlen=max_len, sequences=y_te, value=ner_to_ix[\"pad\"], padding='post', truncating='post')\n",
    "y_te = [to_categorical(i, num_classes=n_tag) for i in y_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Input\n",
    "word_in = Input(shape=(max_len,), name='word_input_')\n",
    "\n",
    "# Word Embedding Using Thai2Fit\n",
    "word_embeddings = Embedding(input_dim=n_thai2dict,\n",
    "                            output_dim=400,\n",
    "                            weights = [thai2fit_weight],input_length=max_len,\n",
    "                            mask_zero=False,\n",
    "                            name='word_embedding', trainable=False)(word_in)\n",
    "\n",
    "# Character Input\n",
    "char_in = Input(shape=(max_len, max_len_char,), name='char_input')\n",
    "\n",
    "# Character Embedding\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars, output_dim=char_embedding_dim, \n",
    "                           input_length=max_len_char, mask_zero=False))(char_in)\n",
    "\n",
    "# Character Sequence to Vector via BiLSTM\n",
    "char_enc = TimeDistributed(Bidirectional(LSTM(units=character_LSTM_unit, return_sequences=False, recurrent_dropout=lstm_recurrent_dropout)))(emb_char)\n",
    "\n",
    "\n",
    "# Concatenate All Embedding\n",
    "all_word_embeddings = concatenate([word_embeddings, char_enc])\n",
    "all_word_embeddings = SpatialDropout1D(0.3)(all_word_embeddings)\n",
    "\n",
    "# Main Model BiLSTM\n",
    "main_lstm = Bidirectional(LSTM(units=main_lstm_unit, return_sequences=True,\n",
    "                               recurrent_dropout=lstm_recurrent_dropout))(all_word_embeddings)\n",
    "main_lstm = TimeDistributed(Dense(50, activation=\"relu\"))(main_lstm)\n",
    "\n",
    "# CRF\n",
    "crf = CRF(n_tag)  # CRF layer\n",
    "out = crf(main_lstm)  # output\n",
    "\n",
    "# Model\n",
    "model = Model([word_in, char_in], out)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=crf_loss, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-53bff9dfcd47>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    BATCH_SIZE = 512  # Number of examples used in each iteration\n",
    "    EPOCHS = 5  # Number of passes through entire dataset\n",
    "    MAX_LEN = 75  # Max length of review (in words)\n",
    "    EMBEDDING = 40  # Dimension of word embedding vector\n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 5\n",
    "    MAX_LEN = 75\n",
    "    EMBEDDING = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1d968b8f7aad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_contrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCRF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Model definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n\u001b[1;32m      7\u001b[0m                   input_length=MAX_LEN, mask_zero=True)(input)  # default: 20-dim embedding\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[1;32m   1455\u001b[0m                              \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m                              input_tensor=tensor)\n\u001b[0m\u001b[1;32m   1458\u001b[0m     \u001b[0;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[0;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'input'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "# Model definition\n",
    "input = Input(shape=(MAX_LEN,))\n",
    "model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=True)(input)  # default: 20-dim embedding\n",
    "model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{val_crf_viterbi_accuracy:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_crf_viterbi_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit([X_word_tr,\n",
    "                     np.array(X_char_tr).reshape((len(X_char_tr), max_len, max_len_char))\n",
    "                     ],\n",
    "                     np.array(y_tr),\n",
    "                     batch_size=train_batch_size, epochs=train_epochs, verbose=1,callbacks=callbacks_list,\n",
    "                     validation_data=(\n",
    "                     [X_word_te,\n",
    "                     np.array(X_char_te).reshape((len(X_char_te), max_len, max_len_char))\n",
    "                     ],\n",
    "                     np.array(y_te))\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
